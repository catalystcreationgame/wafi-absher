# backend/services/llm_service.py
import os
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

class LLMService:
    def __init__(self, config):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.llm = None
        self.initialize_model()
    
    def initialize_model(self):
        """Initialize the ALLaM model with HuggingFace"""
        try:
            print("ğŸ”„ Loading ALLaM-7B model...")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.HF_MODEL,
                trust_remote_code=True,
                use_auth_token=self.config.HF_API_KEY
            )
            
            # Load model with quantization for efficiency
            device = 0 if torch.cuda.is_available() and self.config.LLM_DEVICE == 'cuda' else -1
            
            text_gen_pipeline = pipeline(
                "text-generation",
                model=self.config.HF_MODEL,
                tokenizer=self.tokenizer,
                device=device,
                max_new_tokens=self.config.LLM_MAX_TOKENS,
                temperature=self.config.LLM_TEMPERATURE,
                top_p=0.9,
                do_sample=True,
                trust_remote_code=True,
                model_kwargs={
                    "load_in_8bit": True,
                    "device_map": "auto"
                }
            )
            
            self.llm = HuggingFacePipeline(model_pipeline=text_gen_pipeline)
            print("âœ… Model loaded successfully")
            
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            raise
    
    def generate_response(self, user_input, context="", service_type=None):
        """Generate response using the LLM"""
        
        system_prompt = self._get_system_prompt(service_type)
        
        prompt_template = PromptTemplate(
            input_variables=["context", "input"],
            template="""Ø£Ù†Øª ÙˆØ§ÙÙŠ Ø£Ø¨Ø´Ø±ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©.
            
Ø§Ù„Ø³ÙŠØ§Ù‚: {context}

Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {input}

Ø§Ù„Ø±Ø¯ (Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ø© ÙˆØ§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©):"""
        )
        
        chain = LLMChain(llm=self.llm, prompt=prompt_template)
        
        try:
            response = chain.run(context=context, input=user_input)
            return response.strip()
        except Exception as e:
            print(f"Error generating response: {e}")
            return "Ù…Ø¹Ø°Ø±Ø©ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
    
    def _get_system_prompt(self, service_type):
        """Get service-specific system prompt"""
        
        service_prompts = {
            "photo_change": """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù„Ù„Ø®Ø¯Ù…Ø©: ØªØºÙŠÙŠØ± ØµÙˆØ±Ø© Ø§Ù„Ø¥Ù‚Ø§Ù…Ø©.
- Ø§Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªØ£ÙƒÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø©
- ØªØ­Ù‚Ù‚ Ù…Ù† Ø´Ø±ÙˆØ· Ø§Ù„ØµÙˆØ±Ø© (Ù…Ù„ÙˆÙ†Ø©ØŒ Ø®Ù„ÙÙŠØ© Ø¨ÙŠØ¶Ø§Ø¡ØŒ ÙˆØ§Ø¶Ø­Ø©)
- Ø§Ø·Ù„Ø¨ ØªØ£ÙƒÙŠØ¯Ø§Ù‹ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹""",
            
            "name_change": """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù„Ù„Ø®Ø¯Ù…Ø©: ØªØºÙŠÙŠØ± Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø£ÙˆÙ„.
- Ø§Ø·Ù„Ø¨ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯
- Ø£Ø¹Ø¯ ØªØ£ÙƒÙŠØ¯ Ø§Ù„ØªØºÙŠÙŠØ±
- Ø§Ø·Ù„Ø¨ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ""",
            
            "license_renewal": """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù„Ù„Ø®Ø¯Ù…Ø©: ØªØ¬Ø¯ÙŠØ¯ Ø±Ø®ØµØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©.
- Ø§Ø·Ù„Ø¨ Ø¹Ø¯Ø¯ Ø³Ù†ÙˆØ§Øª Ø§Ù„ØªØ¬Ø¯ÙŠØ¯ (2ØŒ 5ØŒ Ø£Ùˆ 10)
- Ø£Ø¹Ø¯ Ø§Ù„ØªØ£ÙƒÙŠØ¯
- Ø§Ø·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„
- ÙˆØ§ÙÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØ¹""",
            
            "vehicle_sale": """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù„Ù„Ø®Ø¯Ù…Ø©: Ø¨ÙŠØ¹ Ù…Ø±ÙƒØ¨Ø©.
- Ø§Ø·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø© (Ø§Ù„Ù„ÙˆØ­Ø©ØŒ Ø§Ù„Ù†ÙˆØ¹ØŒ Ø§Ù„Ø³Ø¹Ø±)
- Ø§Ø·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙŠ
- Ø£Ø¹Ø¯ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
- Ø§Ø·Ù„Ø¨ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ""",
            
            "default": """Ø£Ù†Øª ÙˆØ§ÙÙŠ Ø£Ø¨Ø´Ø±ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©.
- ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ø© ÙˆØ§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
- ÙƒÙ† ÙˆØ¯ÙŠØ§Ù‹ ÙˆØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
- Ø§Ø·Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ÙˆØ¶ÙˆØ­
- Ø£Ø¹Ø¯ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
- Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…ÙˆØ² Ø§Ù„Ø­Ø§Ù„Ø© ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©"""
        }
        
        return service_prompts.get(service_type, service_prompts["default"])

class ServiceDetector:
    """Detect which service the user is requesting"""
    
    KEYWORDS = {
        "photo_change": ["ØµÙˆØ±Ø©", "Ø§Ù„Ø§Ù‚Ø§Ù…Ø©", "ØªØµÙˆÙŠØ±", "photo"],
        "name_change": ["Ø§Ø³Ù…", "ØªØºÙŠÙŠØ±", "name", "Ø§ÙˆÙ„"],
        "plate_purchase": ["Ù„ÙˆØ­Ø©", "Ø±Ù‚Ù… Ø§Ù„Ù„ÙˆØ­Ø©", "plate", "Ø´Ø±Ø§Ø¡"],
        "parking_report": ["ÙˆÙ‚ÙˆÙ", "Ø®Ø§Ø·Ø¦", "parking", "Ù…Ù‚ÙÙ„"],
        "accident_report": ["Ø­Ø§Ø¯Ø«", "Ø®Ø¯Ø´", "accident", "Ø§ØµØ·Ø¯Ø§Ù…"],
        "certificate": ["Ø´Ù‡Ø§Ø¯Ø©", "Ø³ÙˆØ§Ø¨Ù‚", "certificate", "Ø®Ù„Ùˆ"],
        "marital_status": ["Ø­Ø§Ù„Ø©", "Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©", "Ù…ØªØ²ÙˆØ¬", "marital"],
        "license_renewal": ["Ø±Ø®ØµØ©", "ØªØ¬Ø¯ÙŠØ¯", "license", "Ø³ÙˆØ§Ù‚Ø©"],
        "vehicle_sale": ["Ø¨ÙŠØ¹", "Ù…Ø±ÙƒØ¨Ø©", "Ø³ÙŠØ§Ø±Ø©", "sell"],
        "vehicle_purchase": ["Ø´Ø±Ø§Ø¡", "Ù…Ø±ÙƒØ¨Ø©", "buy"],
        "vehicle_delivery": ["ØªØ³Ù„ÙŠÙ…", "Ù…Ø±ÙƒØ¨Ø©", "delivery"],
        "vehicle_auth_cancel": ["Ø§Ù„ØºØ§Ø¡", "ØªÙÙˆÙŠØ¶", "cancel"],
        "kafo_service": ["ÙƒÙÙˆ", "ØªÙˆØµÙŠÙ„", "delivery", "apps"],
        "weapon_transfer": ["Ø³Ù„Ø§Ø­", "Ù†Ù‚Ù„", "transfer", "weapon"]
    }
    
    @classmethod
    def detect_service(cls, user_input):
        """Detect service type from user input"""
        input_lower = user_input.lower()
        
        for service, keywords in cls.KEYWORDS.items():
            if any(keyword in input_lower for keyword in keywords):
                return service
        
        return "general"